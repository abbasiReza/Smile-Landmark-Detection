{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "json_files=os.listdir(\"C:/Users/user01/Documents/yasaman_daghighi/jsons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "removes=[]\n",
    "# Open the file in write mode\n",
    "for file in json_files:\n",
    "    with open(\"C:/Users/user01/Documents/yasaman_daghighi/jsons/\"+file, 'r') as f:\n",
    "    # Load the JSON data from the file\n",
    "        data = json.load(f)\n",
    "        # break\n",
    "        if len(data['shapes'])!=26:\n",
    "            print(file)\n",
    "            print(len(data['shapes']))\n",
    "            removes.append(file[:-5])\n",
    "    # print(file[:file.index('.')])\n",
    "    # with open('our_dataset/train/'+file[:file.index('.')]+'.pts', 'w') as f:\n",
    "    #     f.write(\"version: 1\\n\")\n",
    "    #     f.write('n_points:  37\\n')\n",
    "    #     f.write('{\\n')\n",
    "    #     for tag in tags:\n",
    "    #         for a in data['shapes']:\n",
    "    #             if a['label']==tag:\n",
    "                    \n",
    "    #                 f.write('{} {}\\n'.format(a['points'][0][0],a['points'][0][1]))\n",
    "    #     f.write('}\\n')\n",
    "\n",
    "\n",
    "        # Write each line of data to the file\n",
    "        # f.write(\"version: 1\\n\")\n",
    "        # f.write('n_points:  37\\n')\n",
    "        # f.write('{\\n')\n",
    "        # for a in data['shapes']:\n",
    "        \n",
    "        #     f.write('{} {}\\n'.format(a['points'][0][0],a['points'][0][1]))\n",
    "        # f.write('}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in data['shapes']:\n",
    "    if a['label']=='25LP':\n",
    "        print(a['points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for a in data['shapes']:\n",
    "    labels.append(a['label'])\n",
    "\n",
    "len(labels)\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[]\n",
    "for a in data['shapes']:\n",
    "    tags.append(a['label'])\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files=os.listdir('C:/Users/user01/Documents/landmark/landmark detection data/all jsons/test set')\n",
    "len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Open the file in write mode\n",
    "for file in json_files:\n",
    "    with open(\"C:/Users/user01/Documents/yasaman_daghighi/jsons/\"+file, 'r') as f:\n",
    "    # Load the JSON data from the file\n",
    "        data = json.load(f)\n",
    "    print(file[:file.index('.')])\n",
    "    with open(\"C:/Users/user01/Documents/yasaman_daghighi/new_jsons/\"+file[:file.index('.')]+'.pts', 'w') as f:\n",
    "\n",
    "        f.write(\"version: 1\\n\")\n",
    "        f.write('n_points:  26\\n')\n",
    "        f.write('{\\n')\n",
    "        for tag in labels:\n",
    "            for a in data['shapes']:\n",
    "                if a['label']==tag:\n",
    "                    \n",
    "                    f.write('{} {}\\n'.format(a['points'][0][0],a['points'][0][1]))\n",
    "        f.write('}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from torchlm.data import LandmarksWFLWConverter\n",
    "from torchlm.data import Landmarks300WConverter\n",
    "from torchlm.data import LandmarksCOFWConverter\n",
    "from torchlm.data import LandmarksAFLWConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filess = os.listdir('E:/code/landmark/daghigihi/test')\n",
    "for a in filess:\n",
    "    if a.endswith('.pts'):\n",
    "        if a[:-4]+'.jpg' not in filess and  a[:-4]+'.JPG' not in filess:\n",
    "            print(a[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"E:/code/landmark/daghigihi/test\"  # Specify the directory path here\n",
    "\n",
    "# List of prefixes to match file names\n",
    "prefixes = ['487','543','94','103', '133', '153', '158', '196', '218', '270', '405', '418', '44', '443', '446', '468', '477', '478', '493', '509', '524', '550', '614', '65', '663', '671', '71', '73', '77', '78', '81', '92']\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Iterate over the files and remove the ones with matching prefixes\n",
    "for file_name in files:\n",
    "    for prefix in prefixes:\n",
    "        if file_name.startswith(prefix):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed file: {file_path}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_300w_converter():\n",
    "    converter = Landmarks300WConverter(\n",
    "        data_dir=\"E:/code/landmark/daghigihi\",\n",
    "        save_dir=\"E:/code/landmark/daghigihi\",\n",
    "        extend=0.2,\n",
    "        rebuild=True,\n",
    "        target_size=256,\n",
    "        keep_aspect=False,\n",
    "        force_normalize=True,\n",
    "        force_absolute_path=True\n",
    "    )  \n",
    "    converter.convert()\n",
    "\n",
    "    converter.show(count=30)\n",
    "test_300w_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"E:/code/landmark/daghigihi/train.txt\"  # Replace with the actual path to your text file\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read each line in the file\n",
    "    for line in file:\n",
    "        # Process each line here\n",
    "        temp = line.strip()\n",
    "        if len(temp.split(' ')) !=53:\n",
    "            print(temp)  # Example: Print the line (stripping any leading/trailing whitespace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchlm.models import pipnet\n",
    "# import torchlm\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchlm.models import pipnet\n",
    "import torchlm\n",
    "import torch\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from torchlm.models import pipnet\n",
    "# will auto download pretrained weights from latest release if pretrained=True\n",
    "model = pipnet(backbone=\"resnet101\", pretrained=True, num_nb=10, num_lms=68, net_stride=32,\n",
    "               input_size=256, meanface_type=\"300w\", backbone_pretrained=False)\n",
    "\n",
    "\n",
    "# #  \n",
    "model.apply_freezing(backbone=False)\n",
    "# generate your custom meanface.\n",
    "custom_meanface, custom_meanface_string = torchlm.data.annotools.generate_meanface(\n",
    "  annotation_path=\"E:/code/landmark/daghigihi/train2.txt\",\n",
    "  coordinates_already_normalized=True)\n",
    "\n",
    "# setting up your custom meanface\n",
    "model.set_custom_meanface(custom_meanface_file_or_string=custom_meanface_string)\n",
    "model.apply_training(\n",
    "    annotation_path=\"E:/code/landmark/daghigihi/train2.txt\",  # or fine-tuning your custom data\n",
    "    num_epochs=150,\n",
    "    learning_rate=3e-4,\n",
    "    save_dir=\"./save/pipnet\",\n",
    "    save_prefix=\"pipnet-wflw-resnet101\",\n",
    "    save_interval=2,\n",
    "    logging_interval=1,\n",
    "    device=\"cuda\",\n",
    "    coordinates_already_normalized=True,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()\n",
    "torch.save(model.state_dict(),'yas_state_dict2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# model.load_state_dict(torch.load('model_state150.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/code/landmark/our_dataset/train.txt') as f:\n",
    "    lines=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('E:/code/landmark/daghigihi/train.txt',delimiter=' ',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "all_sum=[]\n",
    "for i in range(len(df)):\n",
    "    img = plt.imread(df.iloc[i][0])\n",
    "    x_g=[]\n",
    "    y_g=[]\n",
    "    for j in range(1,len(df.iloc[i]),2):\n",
    "        x_g.append(float(df.iloc[i][j])*256)\n",
    "\n",
    "    for j in range(2,len(df.iloc[i]),2):\n",
    "        y_g.append(float(df.iloc[i][j])*256)\n",
    "\n",
    "    model.apply_detecting(image=img)\n",
    "    xx=torch.load('lms_pred_x.pt')*256\n",
    "    yy=torch.load('lms_pred_y.pt')*256\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for a in xx.cpu():\n",
    "        x.append(a.item())\n",
    "    for a in yy:\n",
    "        y.append(a.item())\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the image using imshow()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Generate some random x and y coordinates\n",
    "    # x = np.random.randint(0, img.shape[1], size=50)\n",
    "    # y = np.random.randint(0, img.shape[0], size=50)\n",
    "\n",
    "    # Plot the points using scatter()\n",
    "    k=20\n",
    "    # ax.scatter(x[k], y[k], c='r')\n",
    "    # ax.scatter(x_g[15], y_g[15], c='g')\n",
    "    ax.scatter(x, y, c='r')\n",
    "    # ax.scatter(x_g, y_g, c='g')\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    import math\n",
    "    import numpy as np\n",
    "    y_idexed=[]\n",
    "    sums=[]\n",
    "    for i in range(len(x)):\n",
    "        min=+9999\n",
    "        index=-1\n",
    "        for j in range(len(x_g)):\n",
    "            # if j in y_idexed:\n",
    "            #     continue\n",
    "            p1=[x[i],y[i]]\n",
    "            p2=[x_g[j],y_g[j]]\n",
    "            if abs(math.dist(p1,p2))<min:\n",
    "                min=abs(math.dist(p1,p2))\n",
    "                index=j\n",
    "        # if index not in y_idexed:\n",
    "        y_idexed.append(index)\n",
    "        sums.append(math.dist([x[i],y[i]],[x_g[index],y_g[index]]))\n",
    "        # print(i,index)\n",
    "    # print(np.mean(sums))\n",
    "    print(len(y_idexed))\n",
    "    all_sum.append(np.mean(sums))\n",
    "print(np.mean(all_sum))\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_result=pd.DataFrame(columns=['file','type'])\n",
    "for tag in labels:\n",
    "    df_result[tag+'_x']=None\n",
    "    df_result[tag+'_y']=None\n",
    "\n",
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "all_sum=[]\n",
    "model.eval()\n",
    "outputs=[]\n",
    "for i in range(len(df)):\n",
    "    if i==3000:\n",
    "        break\n",
    "    img = plt.imread(df.iloc[i][0])\n",
    "    x_g=[]\n",
    "    y_g=[]\n",
    "    for j in range(1,len(df.iloc[i]),2):\n",
    "        x_g.append(float(df.iloc[i][j])*256)\n",
    "\n",
    "    for j in range(2,len(df.iloc[i]),2):\n",
    "        y_g.append(float(df.iloc[i][j])*256)\n",
    "\n",
    "    output=model.apply_detecting(image=img)\n",
    "    xx=torch.load('lms_pred_x.pt')*256\n",
    "    yy=torch.load('lms_pred_y.pt')*256\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for a in xx.cpu():\n",
    "        x.append(a.item())\n",
    "    for a in yy:\n",
    "        y.append(a.item())\n",
    "    outputs.append(output)\n",
    "    x2=output[:,0]\n",
    "    y2=output[:,1]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the image using imshow()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Generate some random x and y coordinates\n",
    "    # x = np.random.randint(0, img.shape[1], size=50)\n",
    "    # y = np.random.randint(0, img.shape[0], size=50)\n",
    "\n",
    "    # Plot the points using scatter()\n",
    "    k=20\n",
    "    # ax.scatter(x[k], y[k], c='r')\n",
    "    # ax.scatter(x_g[15], y_g[15], c='g')\n",
    "    ax.scatter(x, y, c='r')\n",
    "    ax.scatter(x_g, y_g, c='g')\n",
    "    temp=[]\n",
    "    temp.append(df.iloc[i][0][df.iloc[i][0].rfind('\\\\')+1:])\n",
    "    temp.append('ground_truth')\n",
    "    for r in range(len(x_g)):\n",
    "        temp.append(x_g[r])\n",
    "        temp.append(y_g[r])\n",
    "    # df_result = df_result.append(pd.Series(temp), ignore_index=True)\n",
    "    df_result.loc[len(df_result)]=temp\n",
    "\n",
    "    temp=[]\n",
    "    temp.append(df.iloc[i][0][df.iloc[i][0].rfind('\\\\')+1:])\n",
    "    temp.append('model_prediction')\n",
    "    for r in range(len(x)):\n",
    "        temp.append(x[r])\n",
    "        temp.append(y[r])\n",
    "    # df_result = df_result.append(pd.Series(temp), ignore_index=True)\n",
    "    df_result.loc[len(df_result)]=temp\n",
    "\n",
    "    \n",
    "    \n",
    "    # with open('E:/code/landmark/our_dataset/landmarks.txt', 'a') as file:\n",
    "    # # Write content to the file\n",
    "    #     file.write(\"{}\\n\".format(df.iloc[i][0]))\n",
    "    #     for j in range(len(x)):\n",
    "    #         file.write('ground_truth:\\n')\n",
    "    #         file.write('{} {}\\n'.format(x_g[j],y_g[j]))\n",
    "            \n",
    "    #         file.write('model predict:\\n')\n",
    "    #         file.write('{} {}\\n'.format(x[j],y[j]))\n",
    "\n",
    "    #         file.write('\\n')\n",
    "\n",
    "\n",
    "    plt.savefig(\"E:/code/landmark/daghigihi/results/{}.png\".format(i))\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "#     import math\n",
    "#     import numpy as np\n",
    "#     y_idexed=[]\n",
    "#     sums=[]\n",
    "#     for i in range(len(x)):\n",
    "#         min=+9999\n",
    "#         index=-1\n",
    "#         for j in range(len(x_g)):\n",
    "#             # if j in y_idexed:\n",
    "#             #     continue\n",
    "#             p1=[x[i],y[i]]\n",
    "#             p2=[x_g[j],y_g[j]]\n",
    "#             if abs(math.dist(p1,p2))<min:\n",
    "#                 min=abs(math.dist(p1,p2))\n",
    "#                 index=j\n",
    "#         # if index not in y_idexed:\n",
    "#         y_idexed.append(index)\n",
    "#         sums.append(math.dist([x[i],y[i]],[x_g[index],y_g[index]]))\n",
    "#         # print(i,index)\n",
    "#     # print(np.mean(sums))\n",
    "#     print(len(y_idexed))\n",
    "#     all_sum.append(np.mean(sums))\n",
    "# print(np.mean(all_sum))\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['file'] = df_result['file'].apply(lambda x : x[x.find('_')+1:])\n",
    "df_result.to_csv('df_res_dgh3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(),'model_state_150_resnet101v2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['file_name', 'width', 'height', \"Tr'_x\", \"Tr'_y\", \"Ft'R_x\", \"Ft'R_y\",\n",
    "       \"Ft'L_x\", \"Ft'L_y\", \"MBR_x\", \"MBR_y\", 'MBL_x', 'MBL_y', 'EAR_x',\n",
    "       'EAR_y', 'EAL_x', 'EAL_y', 'ExR_x', 'ExR_y', 'ExL_x', 'ExL_y', 'EnR_x',\n",
    "       'EnR_y', 'EnL_x', 'EnL_y', 'PupL_x', 'PupL_y', 'PupR_x', 'PupR_y',\n",
    "       'MfR_x', 'MfR_y', 'MfL_x', 'MfL_y', 'MER_x', 'MER_y', 'MEL_x', 'MEL_y',\n",
    "       \"Zy'R_x\", \"Zy'R_y\", \"Zy'L_x\", \"Zy'L_y\", 'Prn_x', 'Prn_y', 'AlR_x', 'AlR_y',\n",
    "       'AlL_x', 'AlL_y', 'Sn_x', 'Sn_y', 'Ls_x', 'Ls_y', 'CpR_x', 'CpR_y',\n",
    "       'CpL_x', 'CpL_y', 'Li_x', 'Li_y', 'Sbl_x', 'Sbl_y', 'ChR_x', 'ChR_y',\n",
    "       'ChL_x', 'ChL_y', \"Me'_x\", \"Me'_y\", \"Go'R_x\", \"Go'R_y\", \"Go'L_x\",\n",
    "       \"Go'L_y\", \"G'_x\", \"G'_y\", \"N'_x\", \"N'_y\", \"Sts_x\", 'Sts_y', 'Sti_x',\n",
    "       'Sti_y', 'St_x', 'St_y']\n",
    "\n",
    "pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['shapes'][10]['label'] in cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in data['shapes']:\n",
    "    print(cols.index(a['label']+'_x'))\n",
    "    print(cols.index(a['label']+'_y'))\n",
    "\n",
    "    # if a['label']+'_x' not in cols:\n",
    "        # print(a['label']+'_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( \"0.09344842654783869 0.15406877348086334 0.9323471108969382 0.1401049083310136 0.3997603796160685 0.15490567536186625 0.6237975504925628 0.1506668314387944 0.23501464872024747 0.10114569063499716 0.23657257629382072 0.14273999515886135 0.7844072096492979 0.09255827995641538 0.7886209940367209 0.13342958152383141 0.1702434206343547 0.2398254983581864 0.8642419047335136 0.23100885422325879 0.3779708093247684 0.24535590755251818 0.6567150531285483 0.24095057462080818 0.27793516230173154 0.20089460792973152 0.27633082606975745 0.26563845764538907 0.7570995084865105 0.19361548230508704 0.7635889825823677 0.2583984281705357 0.279266369046543 0.2302294117140321 0.7575495684331521 0.22349084426991003 0.38280645505717537 0.4925663990251028 0.6614317708106371 0.488658165265478 0.5189481268032609 0.4708399901570863 0.5225011032592264 0.539587476025657 0.32470035972074895 0.6648848905553355 0.7295419104987368 0.6593261745721221 0.5229449400615745 0.6265737161799486 0.5255912932309097 0.662324568779506 0.527155537232314 0.6983437257351308 0.5293851188286474 0.7515984253648098 0.5353511486450372 0.9458980717418055\".split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget  -qO- bench.sh| bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BRACS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
